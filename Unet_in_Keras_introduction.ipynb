{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"Unet_in_Keras_introduction.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WfCbVhnMryN9"},"source":["#Overview\n","\n","In this introductory notebook we will set up, examine, and run a Keras Unet network and train it to segment tomography imaging. The example is kept simple, and in *Further Topics* notebook other choices for training, loss functions and data preparation are considered.\n","\n","\n","**Notes**\n","\n","Blocks of code can be run by clicking the *Play* button or selecting the cell and *Ctrl-Enter*. \n","Also, *Shift-Enter* will run a cell then move the selection to the next cell.\n","\n","\n","**Keras Unet resource links**\n","\n","https://pypi.org/project/keras-unet/\n","\n","https://github.com/karolzak/keras-unet\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WXqX5QFCEdaZ"},"source":["# Setup\n","\n","**Connecting to a GPU**\n","\n","In the *Runtime* menu, select *Change runtime type*. \n","Then select *GPU* as the hardware type.\n","\n","\n","Then check that a GPU is available and what version of Tensorflow is running via the following code:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"pw-5x7zkEdaZ"},"source":["import tensorflow as tf\n","\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","\n","if gpus:\n","    try:\n","        # Currently, memory growth needs to be the same across GPUs\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        # Memory growth must be set before GPUs have been initialized\n","        print(e)\n","    \n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(\"Tensorflow version: \",tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Rl9bg0SEdaa"},"source":["Install keras-unet"]},{"cell_type":"code","metadata":{"id":"Px5DFmETEdaa"},"source":["!pip install keras-unet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1U02d367Edab"},"source":["Some other useful libraries"]},{"cell_type":"code","metadata":{"id":"XAGTD-wqEdab","executionInfo":{"status":"ok","timestamp":1626749391565,"user_tz":-600,"elapsed":295,"user":{"displayName":"Doc Nick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLZE9h2bNF9e-zCZPDhGBcaSrN6JA_aLG1vgeE=s64","userId":"11936445242464571574"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import glob\n","import os\n","import sys\n","from PIL import Image"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VZftFuOeEdab"},"source":["# Create a custom keras_unet model"]},{"cell_type":"code","metadata":{"id":"-007m1dGEdab","executionInfo":{"status":"ok","timestamp":1626749403156,"user_tz":-600,"elapsed":884,"user":{"displayName":"Doc Nick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLZE9h2bNF9e-zCZPDhGBcaSrN6JA_aLG1vgeE=s64","userId":"11936445242464571574"}}},"source":["from keras_unet.models import custom_unet\n","from tensorflow.keras.optimizers import *\n","\n","# size of the input image to U-Net network\n","xsize = 128 \n","ysize = 128\n","\n","model_custom = custom_unet(\n","    input_shape=(xsize, ysize, 1),   \n","    use_batch_norm=False,        # whether to normalise image intensities in batches during training\n","    num_classes=1,               # number of types of object that we are trying to identify\n","    filters=64,                  # how many convolution filters to have at each level\n","    dropout=0.2,                 # how much dropout to use to avoid overfitting\n","    output_activation='sigmoid') # shape of the output function\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"TyPyN5pSEdac"},"source":["model_custom.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l9Skqg2EEdac"},"source":["tf.keras.utils.plot_model(model_custom, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"id":"egqPWc6AEdac"},"source":["# Get some image data for training\n","\n","The data sets we will be using are images and and ground truth masks from the ISCB 2015 segmentation competition. \n","The images are created by electron tomography of cells. \n","The aim is to be able to define the boundaries of structures in the cell automatically. Researchers currently spend many hours and days tracing these structures by hand. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"hA7vOmf4QoEX"},"source":["First, you will need to authorise and mount your google drive from Colab.\n"]},{"cell_type":"code","metadata":{"id":"FDGZrXmSEdac"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llgM3-poGYOz"},"source":["img_files = glob.glob(\"/content/gdrive/My Drive/Colab Notebooks/adv-ML-image-segmentation-with-UNET/data/isbi2015/train/image/*.png\")\n","mask_files = glob.glob(\"/content/gdrive/My Drive/Colab Notebooks/adv-ML-image-segmentation-with-UNET/data/isbi2015/train/label/*.png\")\n","\n","print(\"Found files:\")\n","print(\"Images: \", img_files)\n","print(\"Masks: \",mask_files)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6y2lZr-P7sg"},"source":["Lets have a look at an image and mask from the set. Note that the source images are 512x512 pixels.\n"," "]},{"cell_type":"code","metadata":{"id":"vTrRy0O9Edac"},"source":["img = plt.imread(img_files[0])\n","mask = plt.imread(mask_files[0])\n","\n","f, axarr = plt.subplots(1,2)\n","axarr[0].imshow(img, cmap='gray')\n","axarr[1].imshow(mask, cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8072HEPeQxMt"},"source":["Read in the images, rescale to 128x128, convert to numpy arrays and normalise intensities to a 0..1 range. Then have a look at the shape of the data.\n"]},{"cell_type":"code","metadata":{"id":"DK1vCV9TEdad"},"source":["imgs_list = []\n","masks_list = []\n","\n","for image, mask in zip(img_files, mask_files):\n","    \n","    # we are typing the images as numpy arrays\n","    # for use in tensorflow\n","    # and resizing to (xsize,ysize)\n","    imgs_list.append(np.array(Image.open(image).resize((xsize,ysize),resample=0))) \n","    masks_list.append(np.array(Image.open(mask).resize((xsize,ysize),resample=0))) \n","\n","# normalise image intensity to range 0..1\n","# and mask binary 0,1    \n","x = np.asarray(imgs_list, dtype=np.float32)/255   \n","y = np.asarray(masks_list, dtype=np.float32)/255  \n","\n","print(\"Image array shapes: \",x.shape, y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IqDIJXLBRA-b"},"source":["Reshape the arrays. While we are only using single input/output channels images here, in some cases we might have multiple."]},{"cell_type":"code","metadata":{"id":"0NX4hFmtEdad"},"source":["x = x.reshape(x.shape[0], x.shape[1], x.shape[2], 1)\n","y = y.reshape(y.shape[0], y.shape[1], y.shape[2], 1)\n","\n","print(\"Image array shapes: \",x.shape, y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMqrHdTVRPXP"},"source":["Split the data into training and validation sets.\n"]},{"cell_type":"code","metadata":{"id":"TG4TTpRcEdad"},"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=0)\n","\n","print(\"Training set size:   \", x_train.shape[0])\n","print(\"Validation set size: \", x_val.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHkalFJKEdad"},"source":["# Create a batch image generator for training"]},{"cell_type":"markdown","metadata":{"id":"BXG3RLTxRoH3"},"source":["Create a batch generator to sample images in batches to train on. We allow data augmentation to extend the data sets to include simple transformations on the original training images."]},{"cell_type":"code","metadata":{"id":"8Jm3ha4CEdae"},"source":["from keras_unet.utils import get_augmented\n","\n","train_gen = get_augmented(\n","    \n","    x_train, y_train, batch_size=5,\n","    \n","    data_gen_args = dict(\n","        rotation_range=15.,\n","        width_shift_range=0.05,\n","        height_shift_range=0.05,\n","        shear_range=30,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        vertical_flip=True,\n","        fill_mode='constant'\n","    ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THiqSODkRdOW"},"source":["Lets look at a batch of images. Here we use the keras_unet utility function plot_imgs"]},{"cell_type":"code","metadata":{"id":"-a5-TKI0Edae"},"source":["sample_batch = next(train_gen)\n","x_batch, y_batch = sample_batch\n","print(x_batch.shape, y_batch.shape)\n","\n","from keras_unet.utils import plot_imgs\n","plot_imgs(org_imgs=x_batch, mask_imgs=y_batch, nm_img_to_plot=5, figsize=6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TIekwxlJZG3q"},"source":["#Compiling and training the model "]},{"cell_type":"markdown","metadata":{"id":"dEQv8IK9R9Y_"},"source":["Compile the model. In the other notebook we will consider different optimisers and loss functions."]},{"cell_type":"code","metadata":{"id":"53Mts3n4Sf9s"},"source":["model_custom.compile(optimizer = Adam(learning_rate = 1e-4), \n","                     #optimizer=SGD(lr=0.01, momentum=0.99),\n","                     loss = 'binary_crossentropy', \n","                     metrics = ['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O8uP_RHnSkJc"},"source":["Train the model for 20 epochs."]},{"cell_type":"code","metadata":{"id":"DSABX7X2N6m5"},"source":["history = model_custom.fit(train_gen,\n","                 steps_per_epoch=100,\n","                 epochs=20, \n","                 validation_data=(x_val,y_val)\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vpoX7r29SaNM"},"source":["Save the trained model so can be loaded later."]},{"cell_type":"code","metadata":{"id":"LMSDa26uEdae","executionInfo":{"status":"ok","timestamp":1626750476598,"user_tz":-600,"elapsed":1459,"user":{"displayName":"Doc Nick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLZE9h2bNF9e-zCZPDhGBcaSrN6JA_aLG1vgeE=s64","userId":"11936445242464571574"}}},"source":["model_custom.save('unet_custom_membrane.hdf5')"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpll_qPNS4hK"},"source":["Have a look at the training and validation accuracies. Note that the training accuracy is worse than the validation accuracy. Why?"]},{"cell_type":"code","metadata":{"id":"1bk_7dyHEdaf"},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OP1JztCIeUp6"},"source":["Why do you think the training accuracy is so much lower than the validation accuracy?"]},{"cell_type":"markdown","metadata":{"id":"8mlDDbQmSAJH"},"source":["#Look at predictions on some data that the predictor has not been seen"]},{"cell_type":"markdown","metadata":{"id":"B_sw3QRSUmao"},"source":["Read in and predict on the test images."]},{"cell_type":"code","metadata":{"id":"KGbH-cDkEdaf","executionInfo":{"status":"ok","timestamp":1626750541739,"user_tz":-600,"elapsed":12098,"user":{"displayName":"Doc Nick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLZE9h2bNF9e-zCZPDhGBcaSrN6JA_aLG1vgeE=s64","userId":"11936445242464571574"}}},"source":["test_img_files = glob.glob(\"/content/gdrive/My Drive/Colab Notebooks/adv-ML-image-segmentation-with-UNET/data/isbi2015/test/*.png\")\n","\n","imgs_list = []\n","masks_list = []\n","\n","for image in test_img_files:  \n","    \n","    test_img = np.array(Image.open(image).resize((xsize,ysize),resample=0)) \n","    imgs_list.append(test_img)\n","    \n","    x_test = np.asarray(test_img, dtype=np.float32)/255  \n","    x_test = x_test.reshape(1,x_test.shape[0], x_test.shape[1], 1)\n","    result = model_custom.predict(x=x_test,verbose=0)\n","    result = result.reshape(result.shape[1], result.shape[2])\n","    masks_list.append(result)\n","\n","x = np.asarray(imgs_list, dtype=np.float32)   \n","y = np.asarray(masks_list, dtype=np.float32)   "],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXK8CTGIUty5"},"source":["Have a look at an image and the prediction on that image."]},{"cell_type":"code","metadata":{"id":"GajrKljkEdaf"},"source":["f, axarr = plt.subplots(1,2)\n","n=1\n","axarr[0].imshow(x[n], cmap='gray')\n","axarr[1].imshow(y[n], cmap='gray')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbIebUi7U1TV"},"source":["Notice that the prediction image is not binary. The output of the predictor is a \"probability\" map. So lets binarise the probilities to create a segementation. "]},{"cell_type":"code","metadata":{"id":"_7uL_WyiEdag"},"source":["f, axarr = plt.subplots(1,2)\n","n=1\n","binarise = 1.0 * ( y[n] < 0.5)\n","axarr[0].imshow(x[n], cmap='gray')\n","axarr[1].imshow(binarise, cmap='gray')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2szmWDjEdag"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79igA3kGt4Mr"},"source":["#Batch example without augmentation\n","\n","The following code shows how to use load all the data at once and have the fit function create batches on the fly. While simpler to implement, the disadvantage is that you can not do data augmentation this way."]},{"cell_type":"code","metadata":{"id":"_lz07mSUEdag"},"source":["model_custom_noaug = custom_unet(\n","    input_shape=(xsize, ysize, 1),   \n","    use_batch_norm=False,        \n","    num_classes=1,               \n","    filters=64,                  \n","    dropout=0.2,                 \n","    output_activation='sigmoid')\n","\n","model_custom_noaug.compile(optimizer = Adam(learning_rate = 1e-4), \n","                     loss = 'binary_crossentropy', \n","                     metrics = ['accuracy'])\n","\n","history = model_custom.fit(\n","    x_train,\n","    y_train,\n","    batch_size=5,\n","    epochs=20,\n","    validation_data=(x_val, y_val),\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1sapCOcFEdag"},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-kUUngheyt-"},"source":["What do you notice about the training and validation accuracies here in comparison to when we trained with data augmentation?"]}]}